{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "#explain{\n",
       "    color:white;background-color:black;padding:10px;\n",
       "    margin: 10px 0px 10px 0px;\n",
       "}\n",
       "#yellow{color:#4AFD32}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "#explain{\n",
    "    color:white;background-color:black;padding:10px;\n",
    "    margin: 10px 0px 10px 0px;\n",
    "}\n",
    "#yellow{color:#4AFD32}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CHAPTER 6\n",
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature\n",
    "<pre id=\"explain\">\n",
    "데이타 개수가 적어도 모델의 신뢰도가 크게 떨어지지 않는다\n",
    "정규화 과정에 상관 없이 결과가 같다\n",
    "데이타에 대한 가정이 거의 없다\n",
    "오버피팅되기 쉽다\n",
    "파라미터의 개수가 모델을 세우기 전까지는 정해지지 않는다\n",
    "    <span id=\"yellow\">Nonparametric Model</span>\n",
    "    <=> <span id=\"yellow\">Parametric Model</span>\n",
    "        파라미터의 개수가 모델 세우기 전에 정해지고 자유도에 제약이 생긴다\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre id=\"explain\">\n",
    "iris 데이타셋에 DecisionTreeClassifier를 적용\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre id=\"explain\">\n",
    "DecisionTree를 시각화\n",
    "output: iris_tree.dot\n",
    "=> dot파일을 png파일로 변환\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "import os\n",
    "os.system(\"dot -Tpng iris_tree.dot -o iris_tree.png\")\n",
    "os.system(\"rm iris_tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Making Predictions\n",
    "<br>\n",
    "<img src=\"1.png\" width=600 style=\"display:inline-block\">\n",
    "<div>\n",
    "<img src=\"iris_tree.png\" width=\"300\" style=\"display:inline-block\">\n",
    "<div id=\"explain\" style=\"display:inline-block;border-radius:30px;\">\n",
    "<p>purity</p>\n",
    "<p><span id=\"yellow\">$P_{i,k}$</span> : i번째 노드에서 k클래스의 비율</p>\n",
    "<p>gini:</p>\n",
    "<p><span id=\"yellow\">$G_i = 1 - \\sum_{k=1}^{n}P_{i,k}^2$</span></p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<pre id=\"explain\">\n",
    "1.  루트노드에서 시작 (depth 0)\n",
    "2.  루트노드에서 petal length가 2.45cm 보다 작은지 확인 \n",
    "    <span id=\"yellow\">=> setosa</span>\n",
    "3.  작으면 왼쪽자식노드, 크면 오른쪽 자식노드 (depth 1)\n",
    "4.  오른쪽 자식노드에서 petal width가 1.75cm보다 작은지 확인 \n",
    "    <span id=\"yellow\">=> versicolor</span>\n",
    "5.  Otherwise \n",
    "    <span id=\"yellow\">=> virginica</span>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre id=\"explain\">\n",
    "의사결정나무는 분류과정이 직관적으로 이해 가능\n",
    "<span id=\"yellow\">=> white box model</span>\n",
    "신경망이나 랜덤포레스트 같은 경우는 이해하기 힘들다\n",
    "<span id=\"yellow\">=> black box model</span>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Estimation Class Probabilties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각각 클래스에 대한 확률:\n",
      "\t[[ 0.          0.90740741  0.09259259]]\n",
      "가장 높은 확률의 클래스:\n",
      "\t[1]\n"
     ]
    }
   ],
   "source": [
    "print(\"각각 클래스에 대한 확률:\\n\\t%s\"%tree_clf.predict_proba([[5, 1.5]]))\n",
    "print(\"가장 높은 확률의 클래스:\\n\\t%s\"%tree_clf.predict([[5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre id=\"explain\">\n",
    "<span id=\"yellow\">각각 클래스에 대한 확률 계산</span>\n",
    "length 5, width 1.5일 경우\n",
    "    setosa일 확률 0\n",
    "    versicolor일 확률 49/54 = 0.907\n",
    "    virginica일 확률 5/54 = 0.093\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The CART Training Algorithm\n",
    "### \"the Classification And Regression Tree\"\n",
    "<pre id=\"explain\">\n",
    "<span id=\"yellow\">꼭 2개의 자식만 있어야 하나?</span>\n",
    " - 써킷은 <span id=\"yellow\">CART</span> 알고리즘을 쓰는데 이게 Binary트리를 지원한다.\n",
    " - 꼭 2개일 필요는 없다. ID3같은 알고리즘을 쓰면 2개 이상의 자식노드도 쓸수 있다.\n",
    "</pre>\n",
    "<div id=\"explain\">\n",
    "<p><span id=\"yellow\">$G_{left/right}$</span>: impurity of left/right subset</p>\n",
    "<p><span id=\"yellow\">$m_{left/right}$</span>: number of instances in the left/right subset</p>\n",
    "<p><span id=\"yellow\">$J(k,t_k) = \\frac{m_{left}}{m}G_{left}+\\frac{m_{right}}{m}G_{right}$</span>: Cost Function</p>\n",
    "</div>\n",
    "\n",
    "<pre id=\"explain\">\n",
    "CART는 그리디 알고리즘이다\n",
    "\n",
    "그리디알고리즘\n",
    "    부분을 최적화 한후 합쳐가면서 적당한 최적점을 찾는다\n",
    "    괜찮은 솔루션을 제시하지만 그게 최적이라고 보장하지는 않는다\n",
    "\n",
    "왜 최적해를 찾지 않고 그리디알고리즘을 쓰는가?\n",
    "    최적의 의사결정나무를 찾는 것은 NP-Complete 문제라고 알려져 있기 때문\n",
    "    시간복잡도가 지수 형태로 연산시간이 너무 오래걸린다\n",
    "    \n",
    "시간복잡도\n",
    "    예측에는 O(log(m))시간 밖에 걸리지 않는다\n",
    "    모델 생성에는 O(nmlog(m))시간이 걸린다\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gini Impurity or Entropy?\n",
    "\n",
    "<p id=\"explain\">\n",
    "서킷에서 파라미터로 설정 가능<br><br>\n",
    "<span id=\"yellow\">$G_i = 1 - \\sum_{k=1}^{n}P_{i,k}^2$</span><br><br>\n",
    "<span id=\"yellow\">$p_{i,k}\\neq0$</span><br>\n",
    "<span id=\"yellow\">$H_i = -\\sum_{k=1}^{n}p_{i,k}log(p_{i,k})$</span>\n",
    "</p>\n",
    "<pre id=\"explain\">\n",
    "Gini impurity: 가장 빈도높은 클래스에 치중되는 경향\n",
    "Entropy: 균형잡힌 의사결정나무를 만드는 편\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regularization Hyperparameters\n",
    "\n",
    "<pre id=\"explain\">\n",
    "오버피팅을 방지하기 위해 Regularization을 한다\n",
    "의사결정 나무에서의 Regularization\n",
    "    나무의 깊이를 제한하기\n",
    "    분류되는 클래스 안의 인스턴스 수의 최소값을 정하기\n",
    "    가장 아래 자식 노드의 개수를 제한하기\n",
    "Pruning\n",
    "    일단 완벽하게 나눈 다음에 카이제곱검정같은 검정을 통해 가지치기 한다\n",
    "    보통 5%를 유의수준으로 둔다\n",
    "</pre>\n",
    "\n",
    "<img src=\"2.png\" width=600 style=\"display:inline-block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Regression\n",
    "### 의사결정 나무로 회귀분석 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)\n",
    "export_graphviz(\n",
    "    tree_reg,\n",
    "    out_file=\"reg_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "import os\n",
    "os.system(\"dot -Tpng reg_tree.dot -o reg_tree.png\")\n",
    "os.system(\"rm reg_tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"reg_tree.png\" width=300 style=\"display:inline-block\">\n",
    "<img src=\"3.png\" width=300 style=\"display:inline-block\">\n",
    "\n",
    "<pre id=\"explain\">\n",
    "class 대신 value를 예측\n",
    "gini 대신 mse\n",
    "</pre>\n",
    "\n",
    "<img src=\"4.png\" width=600 style=\"display:inline-block\">\n",
    "\n",
    "### cost function\n",
    "\n",
    "<img src=\"5.png\" width=600 style=\"display:inline-block\">\n",
    "\n",
    "### regularization\n",
    "\n",
    "<img src=\"6.png\" width=600 style=\"display:inline-block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Instability\n",
    "\n",
    "<pre id=\"explain\">\n",
    "<span id=\"yellow\">Issue 1</span>\n",
    "의사결정나무는 Orthogonal 한 구분선에 유리하다\n",
    "따라서 사전에 PCA처리를 하기도 한다\n",
    "<img src=\"7.png\" width=600 style=\"display:inline-block\">\n",
    "\n",
    "<span id=\"yellow\">Issue 2</span>\n",
    "의사결정나무는 training data set의 variation에 매우 민감하다\n",
    "파란색 class에서 가장 width가 큰 data 한개를 지우니까 모델이 바뀌었다\n",
    "<img src=\"8.png\" width=600 style=\"display:inline-block\">\n",
    "\n",
    "<span id=\"yellow\">Issue 3</span>\n",
    "알고리즘이 node마다 Stochastic한 성질이 있다\n",
    "=> 같은 training data에 대해서 다른 모델이 생겨날 수 있다\n",
    "</pre>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
