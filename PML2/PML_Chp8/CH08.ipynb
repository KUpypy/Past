{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Applying Machine Learning to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 챕터에서는 **Natural Language Processing (NLP)**의 한 분야인 **sentiment analysis**를 수행할 것이다. 수행할 주제들은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 데이터 클리닝\n",
    "- 텍스트 문서에서 feature vector 만들기\n",
    "- 긍정/부정 영화 리뷰 분류\n",
    "- `out-of-core` learning을 이용한 큰 텍스트 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the IMDb movie review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis는 **opinion mining**이라고도 불린다. 이것은 문서의 **polarity**를 분석한다. 주로 분석하는 대상은 특정 주제에 대한 저자의 의견이나 감정 표현을 분류하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 챕터에서는 **Internet Movie Database (IMDb)**를 사용해 분석을 시행할 것이다. 여기에는 50,000개의 양극으로 나뉜 리뷰가 들어있다(positive or negative). positive가 의미하는 것은 영화 별점이 6점 이상, 그리고 negative는 5개 이하를 의미한다. \n",
    "\n",
    "*출처: http://ai.stanford.edu/~amaas/data/sentiment/ * // 다운로드 후, 해당 디렉토리로 이동해 `tar -zxf aclImdb_v1.tar`명령어로 압축 해제 가능하다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "데이터를 불러와서, 하나의 csv파일로 만들어 준다. 이를 pandas의 `DataFrame` 오브젝트로 만들어 준다. 완료까지의 시간을 측정하고 progress를 보기 위해 **PyPrind**라는 패키지를 사용한다. \n",
    "\n",
    "*출처: https://pypi.python.org/pypi/PyPrind/*  // `pip install pyprind`를 이용해 다운 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:56\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "\n",
    "labels = {'pos':1, 'neg':0}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path ='/Users/WooJin/Documents/ML_study/PML2/PML_Chp8/aclImdb/%s/%s' % (s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r') as infile: #??\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "합쳐진 데이터셋은 이미 정렬 되어있기 때문에 `np.random`의 `permutation`함수를 이용해 데이터들을 섞어줄 것이다. 이는 데이터를 로컬 자체에서 훈련 / 테스트 데이터셋으로 나눠주는데 유용하다. 일단은, 섞어진 데이터를 csv파일로도 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('/Users/WooJin/Documents/ML_study/PML2/PML_Chp8/movie_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/WooJin/Documents/ML_study/PML2/PML_Chp8/movie_data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introducting the bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ch4에서 기억하듯이, 우리는 범주형 데이터를 numerical form으로 바꿔 주어야 한다. 이 섹션에서는 **bag-of-words**라는 모델을 소개할 것이다. 이 모델은 텍스트를 numerical feature vectors로 바꿔주는 역할을 한다. 이 모델의 아이디어는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We create a **vocabulary** of unique **tokens** - for example, words - from the entire set of documents.\n",
    "2. We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 문서의 고유한 단어들은 bag-of-words 단어들의 작은 부분집합으로 나타나, feature vector는 거의 0으로 이루어질 것이다(**sparse**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming words into feautre vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 문서의 단어 카운트에 기반해 bag-of-words 모델을 만들기 위해서 scikit-learn의 `CountVectorizer`클래스를 사용한다. `CountVectorizer`클래스는 텍스트 데이터의 어레이(문서 혹은 하나의 문장)를 받아 bag-of-words 모델을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining and the weather is sweet'])\n",
    "\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`의 `fit_transform`메소드를 통해, 다음 세개의 문장을 sparse feature vectors로 만들어 주었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `The sun is shining`\n",
    "2. `The weather is sweet`\n",
    "3. `The sun is shining and the weather is sweet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_) #각 integer가 feature vector array의 열번호"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 만든 feature vectors를 프린트하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = df['review']\n",
    "\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104082\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_[max(count.vocabulary_, key=count.vocabulary_.get)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature vectors의 이러한 값들은 **raw term frequencies**: tf(t,d) - the number of times a term *t* occurs in a document *d* - 라고도 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   우리가 방금 만든 bag-of-words 모델은 **1-gram**혹은 **unigram**방식이다. 일반적으로, NLP에서 어떤 item의 연속적인 sequence는 **n-gram**이라고 불린다. \n",
    "   - **1-gram**: \"the\", \"sun\", \"is\", \"shining\"\n",
    "   - **2-gram**: \"the sun\", \"sun is\", \"is shining\"\n",
    "   \n",
    "     `CountVectorizer`클래스는 `ngram_range`파라메터로 이 gram을 설정할 수 있게 해준다.       ex) ngram_range(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing word relevancy via term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 데이터를 분석할때, 우리는 여러 문서(negative, positive)에서 반복적으로 발견되는 어휘들에 마주치게 된다. 그러한 어휘들은 보통 유용한 정보를 담고있지 않다. 이 섹션에서는 그러한 어휘들의 가중치를 줄여주는 **term  frequency-inverse document frequency(tf-idf)**라는 테크닉을 학습할 것이다. tf-idf는 **term frequency**와 **inverse document frequency**의 곱으로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_1.png\" alt=\"Drawing\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 tf(t,d)는 term frequency이며, idf(t,d)는 다음과 같이 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"ch8_2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "n_d 는 문서의 총 갯수, df(d,t)는 단어 t를 담고있는 문서의 수 d 이다. 1을 분모에 더해 준것은 선택적이며, 0이 아닌 값을 assign해주기 위해 쓰였다. log는 빈도가 낮은 문서에 가중치를 너무 많이 주지 않기 위해 쓰였다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn은 `CountVectorizer`에서 raw term frequencies를 가져와 tf-idfs로 만들어 주는 `TfidfTransformer`트랜스포머도 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is는 3번째 문서에서 가장 term frequency가 큰 단어였다. 하지만 tf-idfs이후 is가 3번째 문서에서 상대적으로 적은(0.31이 아니라 0.48인듯?) tf-idf를 가진 것을 볼 수 있다. 하지만 우리가 아까의 공식으로 직접 계산하는 것과 `TfidfTransformer`를 써서 계산하는 것의 결과값은 조금 달랐다. scikit-learn에서의 tf-idf와 idf공식은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_3.png\" alt=\"Drawing\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_4.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 tf-idfs를 계산하기 전에 raw term frequencies를 normalize해주는 것 또한 일반적이지만, `TfidfTransformer`는 tf-idfs를 바로 normalizes해준다. 기본적으로(`norm='12'`), scikit-learn의 `TfidfTransformer`는 L2-normalization을 적용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfTransformer`가 어떻게 적용되는지 이해하기 위해 3번째 문서안에 있는 is의 tf-idf를 어떻게 계산했는지 예시를 이용해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`is`는 3번째 문서에서 2의 term frequency를 가지며, `is`의 df(d,t)는 3이다. 따라서, 우리는 다음과 같이 idf를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_6.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_7.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 이 계산을 3번째 문서의 모든 단어들에 적용했을때, 우리는 다음과 같은 tf-idf 벡터를 얻을 수 있다: [1.69, 2.00, 1.29, 1.29, 1.29, 2.00, and 1.29]. 이것은 L2- normalization을 통해 다음과 같이 나타난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_8.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좀더 큰 사이즈로 실험 해보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  X11  X12  X13  X14  X15\n",
       "1  0.19 0.00 0.52 0.00 0.00 0.22 0.00 0.00 0.28 0.00 0.00 0.73 0.00 0.11 0.13\n",
       "2  0.00 0.00 0.23 0.18 0.00 0.00 0.00 0.91 0.00 0.00 0.23 0.00 0.00 0.19 0.00\n",
       "3  0.00 0.50 0.07 0.33 0.21 0.54 0.00 0.14 0.00 0.00 0.00 0.39 0.36 0.00 0.00\n",
       "4  0.50 0.48 0.00 0.00 0.00 0.00 0.37 0.00 0.00 0.00 0.00 0.00 0.12 0.00 0.61\n",
       "5  0.00 0.00 0.00 0.00 0.00 0.00 0.37 0.91 0.00 0.20 0.00 0.00 0.00 0.00 0.00\n",
       "6  0.00 0.00 0.00 0.08 0.00 0.00 0.00 0.00 0.22 0.10 0.00 0.38 0.53 0.35 0.62\n",
       "7  0.00 0.41 0.00 0.27 0.76 0.29 0.21 0.00 0.00 0.23 0.00 0.00 0.00 0.10 0.00\n",
       "8  0.00 0.00 0.50 0.00 0.00 0.43 0.46 0.00 0.55 0.06 0.00 0.00 0.00 0.19 0.11\n",
       "9  0.00 0.00 0.00 0.20 0.00 0.00 0.52 0.00 0.07 0.56 0.06 0.11 0.21 0.00 0.56\n",
       "10 0.00 0.53 0.00 0.24 0.00 0.13 0.00 0.30 0.00 0.00 0.30 0.07 0.57 0.38 0.00\n",
       "11 0.00 0.00 0.22 0.40 0.00 0.43 0.31 0.00 0.36 0.00 0.33 0.31 0.43 0.00 0.00\n",
       "12 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.14 0.00 0.99\n",
       "13 0.00 0.00 0.00 0.30 0.00 0.48 0.17 0.56 0.00 0.00 0.19 0.52 0.16 0.08 0.00\n",
       "14 0.00 0.00 0.15 0.12 0.00 0.26 0.00 0.61 0.00 0.61 0.30 0.00 0.00 0.26 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "docs <- data.frame(matrix(c(round(runif(200, min=0, max=10))), ncol=15, byrow=T))\n",
    "docs[docs == 3 ] <- 0\n",
    "docs[docs == 5 ] <- 0\n",
    "docs[docs == 7 ] <- 0\n",
    "docs[docs == 8 ] <- 0\n",
    "docs[docs == 10 ] <- 0\n",
    "\n",
    "docs['X1'] <- 0\n",
    "docs['X5'] <- 0\n",
    "\n",
    "docs[1,1] <- 1\n",
    "docs[4,1] <- 5\n",
    "\n",
    "docs[3,5] <- 2\n",
    "docs[7,5] <- 9\n",
    "\n",
    "tf_idf <- function(x) {\n",
    "  \n",
    "  # create empty matrix\n",
    "  tf <- data.frame(matrix(, nrow = nrow(x) , ncol = length(x)))\n",
    "  df <- data.frame(matrix(, nrow = nrow(x) , ncol = length(x)))\n",
    "  idf <- data.frame(matrix(, nrow = nrow(x) , ncol = length(x)))\n",
    "  tfidf <- data.frame(matrix(, nrow = nrow(x) , ncol = length(x)))\n",
    "  \n",
    "  n_d <- nrow(x)\n",
    "  \n",
    "  # compute each df, tf, idf and tfidf\n",
    "  for(k in 1:nrow(x)) {\n",
    "    for(i in 1:length(x)) {\n",
    "      \n",
    "      df[k,i] <- nrow(x) - sum(x[,i] == 0)\n",
    "      tf[k,i] <- x[k,i]\n",
    "      idf[k,i] <- log((n_d+1)/(1+df[k,i])) \n",
    "      \n",
    "      tfidf[k,i] <- tf[k,i] * (idf[k,i] + 1)\n",
    "    }\n",
    "  }  \n",
    "  \n",
    "  # L2 normalization\n",
    "  norm <- c()\n",
    "  for(p in 1:nrow(x)) {\n",
    "    \n",
    "    norm[p] <- (sum((tfidf[p, ]^2)) )^(1/2)\n",
    "    \n",
    "    tfidf[p, ] <- tfidf[p, ] / norm[p]\n",
    "  }\n",
    "  \n",
    "  return(tfidf)\n",
    "}\n",
    "\n",
    "round(tf_idf(docs), digits=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 X15\n",
       "1   1  0  4  0  0  2  0  0  2   0   0   6   0   1   1\n",
       "2   0  0  1  1  0  0  0  4  0   0   1   0   0   1   0\n",
       "3   0  6  1  6  2  9  0  2  0   0   0   6   6   0   0\n",
       "4   5  6  0  0  0  0  6  0  0   0   0   0   2   0   9\n",
       "5   0  0  0  0  0  0  4  9  0   2   0   0   0   0   0\n",
       "6   0  0  0  1  0  0  0  0  2   1   0   4   6   4   6\n",
       "7   0  6  0  6  9  6  4  0  0   4   0   0   0   2   0\n",
       "8   0  0  9  0  0  9  9  0  9   1   0   0   0   4   2\n",
       "9   0  0  0  4  0  0  9  0  1   9   1   2   4   0   9\n",
       "10  0  6  0  4  0  2  0  4  0   0   4   1   9   6   0\n",
       "11  0  0  4  9  0  9  6  0  6   0   6   6   9   0   0\n",
       "12  0  0  0  0  0  0  0  0  0   0   0   0   1   0   6\n",
       "13  0  0  0  4  0  6  2  6  0   0   2   6   2   1   0\n",
       "14  0  0  1  1  0  2  0  4  0   4   2   0   0   2   0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 섹션에서 우리는 bag-of-words 모델, term frequencies, 그리고 tf-idfs를 배웠다. 하지만 첫번째 중요한 과정은 텍스트 데이터를 cleaning 하는 것이다(unwanted character때문). 왜 이게 중요한지 예시로 이해해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 볼 수 있듯이, HTML 마크업과 구두점 등이 들어가 있다. 구두점 등은 정보를 이용하는데 유용할 수 있지만, 여기서는 이모티콘을 제외한 모든 구두점과 HTML 마크업 등은 제거할 것이다. 이 일을 하려면, 파이썬의 **regular expression(regex)** library를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + \\\n",
    "                    ' '.join(emoticons).replace('-', '') #? 책에는 '.join으로 나와있음\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 regex <[^>]*> 를 통해서, 우리는 HTML 마크업을 제거하였다. 많은 프로그래머들이 보통 HTML을 parsing하는데 regex를 쓰는 것을 좋아하지 않지만, 이 regex는 데이터셋을 cleaning하는데는 충분하다. HTML 마크업을 제거한 후, 이모티콘을 찾기 위해 `emoticons`라는 다른 regex를 이용했다. 다음은 `[\\W]+`를 이용해 모든 non-word character를 지워 준 후, lowercase로 변환해 `emoticons`에 더해 주었다. 추가적으로, (-)캐릭터 또한 이모티콘에서 지워 주었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: https://developers.google.com/edu/python/regular-expressions, https://docs.python.org/3.4/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이모티콘 캐릭터들을 문서 문자열 뒤에 붙이는 것은 좋은 접근이 아니라고 보일수도 있지만, bag-of-words 모델에서는 문자의 순서가 중요하지 않기 때문에 이렇게 하더라도 큰 문제가 없다. 다음은 위 코드의 실행 샘플이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로, 우리는 `cleaned` 텍스트 데이터를 사용할 것이기 때문에 `preprocessor`함수를 우리의 데이터에 적용시켜준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing documents into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*tokenize*를 위해 할수있는 한가지 방금은 스페이스를 기준으로 나누는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "다른 방법은 **word stemming**이라는 방법이다. 이는 단어를 그것의 root form으로 바꾸어 같은 root(stem)를 가진 단어끼리 mapping할 수있게 해준다. 우리가 여기서 사용하는 알고리즘은 **porter stemmer**이다. 이는  `pip install nltk`로 다운 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK관련 추가 정보: http://www.nltk.org/book/ 에서 확인 가능하다. porter stemmer은 꽤나 오래된 알고리즘이고, 다른 stemming 알고리즘 또한 존재한다. **Snowball stemmer**나 **Lancaster stemmer**가 그것이다. 이것들 또한 NLTK 패키지를 통해 이용 가능하다. (http://www.nltk.org/api/nltk.stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 유용한 주제인 **stop-word removal**에 대해 설명이다. Stop-words란 간단히 말해 모든 종류의 문서에 매우 자주 등장해 유용한 정보를 거의 담고 있지 않은 단어들이다. 예시로, is, and, has 등이 있다. 이러한 stop-words를 제거하는 것은 우리가 tf-idfs가 아닌 raw or normalized term frequencies를 가지고 작업할때 매우 유용하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/WooJin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 훈련 데이터와 테스트 데이터셋을 25,000개씩으로 나눠준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV`를 이용하여 최적의 파라메터 값을 찾아준다. (5-fold stratified cross-validation 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "                'vect__stop_words': [stop, None],\n",
    "                'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                'clf__penalty': ['l1', 'l2'],\n",
    "                'clf__C': [1.0, 10.0, 100.0]},\n",
    "                {'vect__ngram_range': [(1,1)],\n",
    "                'vect__stop_words': [stop, None],\n",
    "                'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                'vect__use_idf':[False],\n",
    "                'vect__norm':[None],\n",
    "                'clf__penalty': ['l1', 'l2'],\n",
    "                'clf__C': [1.0, 10.0, 100.0]}\n",
    "]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                            scoring='accuracy',\n",
    "                            cv=5, verbose=1,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` 오브젝트를 사용할때 여기에서는 컴퓨팅 시간 때문에 파라메터의 경우의 수를 줄여 주었다. 또한 `CountVectorizer`와 `TfidfTransformer`를 `TfidVectorizer`로 바꿔 주었고, 이는 두가지 기능을 모두 수행한다. 여기서는 이전(`use_idf=True, smooth_idf=True, and norm='l2'`)과 달리 `use_idf=False, smooth_idf=False, and norm=None` 을 해 주었는데, 이는 raw term frequencies를 기반으로 모델을 훈련시키기 위함이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x117761d08>} \n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_9.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search에서 찾아낸 파라메터를 가지고 5-fold cross validation accuracy scores를 내어 보면,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV accuracy: 0.897,\n",
    "Test accuracy: 0.899"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers 정보: https://arxiv.org/pdf/1410.5329v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with bigger data - online algorithms and out-of-core learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50,000개의 영화 리뷰 데이터로 feature vectors를 만들어 grid search를 하는건 꽤나 오랜 시간이 걸린다. 현실에서는 우리 컴퓨터의 메모리보다 큰 데이터를 가지고 작업을 하는 상황이 많다. 모든 사람이 슈퍼컴퓨터를 가지고 있지는 않기 때문에, out-or-core learning이라는 테크닉을 배워 볼 것이다. 이는 그러한 큰 데이터를 다룰 수 있게 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챕터2에서, 우리는 **stochastic gradient decent**라는 개념을 다뤘다. 이는 한번에 하나의 샘플을 이용해 모델의 weight를 업데이트 해주었다. 이 섹션에서는 `SGDClassifier`의 `partial_fit`라는 함수를 이용해 우리의 로컬 드라이브에서 바로 문서를 스트림해, small minibatches로 로지스틱 회귀를 훈련시켜볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, `tokenizer`함수를 이용해 우리의 `movie_data.csv`파일의 unprocessed text data를 cleaning 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + \\\n",
    "                    ' '.join(emoticons).replace('-', '') \n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 `stream_docs`라는 generator function을 정의한다. 이는 한번에 한 문서를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='/Users/WooJin/Documents/ML_study/PML2/PML_Chp8/movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는 `get_minibatch`를 통해 문서의 지정된 숫자를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Every new fall line-up show deserves, at least, my \"\"3 strikes and you\\'re out\"\" policy. I give a comedy 3 chances to make me laugh, that is, 3 complete episodes. After Episode 1, I actually said to the TV,\"\"Cancelled tomorrow\"\". It was that bad. I have now watched the first 4 episodes of \"\"Cavemen\"\" and have yet to manage even a smirk. Not a titter, a guffaw, a chortle, as a matter of fact, no facial movement at all. I will continue to punish myself by watching every future episode because I am convinced that I am clearly missing something in this show. I\\'m simply not \"\"getting\"\" it, but I believe that a comedy on a major TV network in prime-time, just HAS to be funny; but there are no laughs from me YET. There\\'s just no way that ABC would put on the least funniest comedy of all time at 8:00 p.m. I KNOW there has got to be an inside joke that just isn\\'t jiving with my brain. I\\'ve read each of the previous comments, I \"\"get\"\" the social aspect of it, but, WHERE ARE THE JOKES ???? I shall continue suffering for at least 30 minutes a week, until I have a light-bulb moment and smack myself in the head shouting \"\"Eureka\"\".\"',\n",
       " 0)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(doc_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불행하게도, 우리는 out-of-core learning에서는 `CountVectorizer`를 사용할 수 없다. 왜냐면 이는 메모리에 모든 데이터를 담아두기 때문이다. 마찬가지로, `TfidVectorizer`또한 그렇다. 하지만 scikit-learn의 `HashingVectorizer`는 데이터와 무관하게 Hashing trick을 이용하기 때문에 이를 이용해 Vectorize해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                            n_features=2**21,\n",
    "                            preprocessor=None,\n",
    "                            tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "\n",
    "doc_stream = stream_docs(path='/Users/WooJin/Documents/ML_study/PML2/PML_Chp8/movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 코드를 이용해, 우리는 `tokenizer`수를 `HashingVectorizer`에 이용하였고, 로지스틱 회귀의 `loss` 파라메터를 `log`로 세팅했다. `HashingVectorizer`의 feature수가 많기 떄문에 우리는 hash collision을 줄일 수 있지만, 동시에 우리 모델의 coefficient 수를 늘리게 된다. --?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:24\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드를 통해 45개의 minibatches(각각 1,000개의 문서)로 훈련시켰다. 남은 5,000개의 문서로 우리 모델의 성능을 평가하면,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 볼 수 있듯, minibatch를 이용한 훈련 결과는 우리가 이전에 수행했던 grid search로 최적화된 모델보다는 더 적은 예측력을 가진다. 하지만, out-of-core learning은 굉장히 효율적이며 더 적은 시간이 걸린다. 마지막으로, 5,000개 문서를 이용해 우리의 모델을 업데이트 시켜준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - bag-of-words모델이 문서의 분류에는 많이 쓰이지만, 문장 구조나 문법은 신경쓰지 못한다. **Latent Dirichlet allocation**은 이를 보완하는 모델이다. 혹은 요즈음엔 **word2vec**을 이용하며, 이는 https://code.google.com/p/word2vec/ 를 참고할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
