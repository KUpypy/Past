{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chp10. Introduction to Artificial Neural Networks\n",
    "\n",
    "- __발표자 : 정지원__\n",
    "- __발표일 : 2017. 7. 22(토)__\n",
    "\n",
    "새들은 우리가 날기 위해 영감을 주었고, 자연은 다른 많은 발명품에 영감을 불어넣었다. 지적인 기계를 만드는 방법에 대한 영감을 얻기 위해 뇌의 구조를 살펴 보는 것은 논리적인것 같다. 이것이 인공 신경망 (Artial Cial Neural Network, ANN)에 영감을 주었던 핵심 아이디어다. 그러나, 비행기가 새들로부터 영감을 받았지만, 날개를 펄럭 일 필요가 없습니다. 비슷하게, ANN은 점차적으로 생물학적 사촌 들과는 아주 다른 모습을 보입니다. 일부 연구자들은 생물학적으로 유추 할 수있는 시스템으로 우리의 창의력을 제한하지 않도록 생물학적 유추를 완전히 삭제해야한다고 주장하기까지합니다 (예 : \"뉴런\"보다는 \"단위\"라고 말함). ANN은 Deep Learning의 핵심입니다. 그들은 이상적 같은 연설에 전원을 공급, 수십억 개의 이미지 (예 : 구글 이미지) 분류 등 크고 매우 복잡한 기계 학습 작업을 해결하기 위해 만드는 상상력 강력하고 다양한, 그리고 scala- recogni- 기 서비스 (예를 들어, 애플의 시리) 최고의 비디오를 추천하는 것은 사용자의 수백만의 수백 시계에 매일 (예 : 유튜브), 또는 자체에 대해 (DeepMind의 AlphaGo)를 재생 한 후 과거 게임의 수백만 검사에 의해 이동의 게임에서 세계 챔피언을 이길 학습.\n",
    "\n",
    "이 장에서는 인공 신경망을 소개하고 처음 ANN 아키텍처를 간략히 살펴 봅니다. 그런 다음 MLP (Multi-Layer Perceptron)를 제시하고 TensorFlow를 사용하여 MNIST 숫자 분류 문제 (3 장에서 소개 함)를 해결합니다.\n",
    "\n",
    "## From Biological to Arti cial Neurons\n",
    "\n",
    "놀랍게도, ANN은 꽤 오랫동안 주변에 있었다 : 그들은 신경 생리학자인 Warren McCulloch와 수학자 Walter Pitts에 의해 1943 년에 처음으로 소개되었다. McCulloch와 Pitts는 획기적인 논리를 사용하여 복잡한 계산을 수행하기 위해 생물학적 뉴런이 동물의 두뇌에서 어떻게 작용할 수 있는지에 대한 간략한 계산 모델을 발표했습니다 (Landke Paper 2, \"신경 활동에 내재하는 논리적 미적분\"). 이것은 최초의 인공 신경 네트워크 아키텍처였습니다. 그 이후로 많은 다른 아키텍처가 발명되었습니다.\n",
    "\n",
    "1960 년대까지 ANN의 초기 성공은 곧 우리가 진정으로 지적인 기계와 대화 할 것이라는 대폭적인 믿음을 이끌어 냈습니다. 이 약속이 실현되지 않을 것이라는 점이 분명 해지자 (적어도 상당 기간 동안) 자금이 다른 곳으로 날아 갔고 ANN은 길게 어두운 시대로 들어갔다. 1980 년대 초에는 새로운 네트워크 아키텍처가 발명되고 더 나은 교육 기술이 개발됨에 따라 ANN에 관심이 다시 생겨났습니다. 그러나 1990 년대에는 지원 벡터 머신 (제 5 장 참조)과 같은 강력한 대체 기계 학습 기술이 더 나은 결과와보다 강력한 이론적 기반을 제공하는 것처럼 보였으므로 대부분의 연구자가 선호했습니다. 마지막으로 ANN에 대한 또 다른 관심의 물결을 목격하고 있습니다. 이전 파도처럼이 파도가 사라질까요? 이 점이 다르다는 것과 우리 삶에 훨씬 더 중대한 영향을 미칠 것이라고 믿을만한 몇 가지 이유가 있습니다.\n",
    "\n",
    "- 1990 년대 이래로 컴퓨팅 능력이 엄청나게 높아지면서 합리적인 시간 내에 대규모 신경 네트워크를 학습 할 수있게되었습니다. 이것은 부분적으로 무어의 법칙에 기인합니다. 또한 게임 산업 덕분에 수백만 명의 강력한 GPU 카드를 생산할 수있었습니다.\n",
    "- 교육 알고리즘이 개선되었습니다. 공평하기 때문에 그들은 1990 년대에 사용 된 것들과 약간 다릅니다 만, 이러한 비교적 작은 비틀기는 엄청난 긍정적 영향을 미칩니다.\n",
    "- ANN의 이론적 한계 중 일부는 실제로는 양성으로 판명되었습니다. 예를 들어, 많은 사람들은 ANN 교육 알고리즘이 지역의 최적 상태에 머물러 있기 때문에 운명에 부딪혔다 고 생각하지만 실제로는 거의 사용되지 않습니다 (또는 실제로 그런 경우 일반적으로 글로벌 최적).\n",
    "- ANN은 자금 조달 및 진보의 선순환으로 들어간 것 같습니다. ANN을 기반으로 한 놀라운 제품은 정기적으로 헤드 라인 뉴스를 작성하여 점점 더 많은주의와 기금을 끌어 당겨 더 많은 제품과 더 놀라운 제품을 만들어냅니다.\n",
    "\n",
    "## Biological Neurons\n",
    "\n",
    "\n",
    "인공 뉴런에 대해 논의하기 전에 생물학적 뉴런을 간단히 살펴 보겠습니다 (그림 10-1 참조). 이것은 주로 동물의 대뇌 피질 (예 : 뇌)에서 발견되는 비정상적인 세포로, 핵과 세포의 복잡한 구성 요소의 대부분을 포함하는 세포체와 수상 돌기라는 많은 분지 확장과 축색 돌기. 축색 돌기의 길이는 세포체보다 몇 배나 길거나 수십만 배까지 길어질 수 있습니다. 사지 가까이에서 축삭 돌기는 텔로 덴드 리아 (telodendria)라고 불리는 많은 가지로 나뉘어지며,이 가지의 끝에는 시냅스 계단 (synaptic terminal) (또는 간단히 시냅스)이라고하는 소소한 구조가 있으며, 다른 곳의 수상 돌기에 연결되어있다 뉴런들. 생물학적 뉴런은이 시냅스를 통해 다른 뉴런의 신호라고 불리는 짧은 전기 충격을받습니다. 뉴런이 수 밀리 초 이내에 다른 뉴런으로부터 충분한 수의 신호를 수신하면 자신의 신호를 발령합니다.\n",
    "\n",
    "![Image](figures/1.png)\n",
    "\n",
    "따라서 개개의 생물학적 뉴런은 오히려 간단한 방식으로 행동하는 것처럼 보이지만 수십억 개의 뉴런에 일반적으로 연결된 각 뉴런의 광대 한 네트워크에서 구성됩니다. 고도로 복잡한 계산은 매우 단순한 뉴런의 방대한 네트워크에 의해 수행 될 수 있습니다. 복잡한 개미가 단순한 개미의 결합 된 노력에서 나타날 수있는 것처럼. 생물학적 신경망 (BNN) 4의 구조는 여전히 활발한 연구 대상이지만 뇌의 일부분이 매핑되어 있으며 그림 10-2에서 볼 수 있듯이 뉴런은 종종 연속적인 층으로 구성되는 것처럼 보입니다.\n",
    "\n",
    "![Image](figures/2.png)\n",
    "\n",
    "## Logical Computations with Neurons\n",
    "\n",
    "Warren McCulloch와 Walter Pitts는 생물학적 뉴런의 매우 간단한 모델을 제안했는데 나중에 인공적인 뉴런으로 알려지게되었습니다. 이것은 하나 이상의 이진 (입력 / 출력) 입력과 하나의 이진 출력을가집니다. 인공 뉴런은 일정 수 이상의 입력이 활성화되었을 때 출력을 활성화합니다. McCulloch와 Pitts는 그러한 단순화 된 모델을 사용해도 원하는 모든 논리적 명제를 계산하는 인공 뉴런 네트워크를 구축하는 것이 가능하다는 것을 보여주었습니다. 예를 들어, 두 개 이상의 입력이 활성화되었을 때 뉴런이 활성화되었다고 가정하고 다양한 논리 연산을 수행하는 몇 개의 ANN을 작성해 봅시다 (그림 10-3 참조).\n",
    "\n",
    "![Image](figures/3.png)\n",
    "\n",
    "\n",
    "- 왼쪽의 첫 번째 네트워크는 단순히 신원 기능입니다. 즉, 뉴런 A가 활성화되면 뉴런 C도 활성화됩니다 (뉴런 A에서 두 개의 입력 신호를 받기 때문에). 그러나 뉴런 A가 오프이면 뉴런 C 꺼져있다.\n",
    "- 두 번째 네트워크는 논리 AND를 수행합니다. 뉴런 C는 뉴런 A와 B가 활성화 된 경우에만 활성화됩니다 (단일 입력 신호로는 뉴런 C를 활성화 할 수 없습니다).\n",
    "- 세 번째 네트워크는 논리 OR을 수행합니다. 신경 A 또는 신경 B가 활성화 된 경우 (또는 두 경우 모두) 신경 C가 활성화됩니다.\n",
    "- 마지막으로, 입력 연결이 뉴런의 활동 (생물학적 뉴런의 경우)을 억제 할 수 있다고 가정하면 네 번째 네트워크는 약간 더 복잡한 논리 명제를 계산합니다. 뉴런 C가 활성화되면 뉴런 C가 활성화되고 뉴런 B가 꺼져 있습니다. 뉴런 A가 항상 활성화되어 있으면 논리적 NOT을 얻습니다. 즉, 뉴런 B가 꺼져있을 때 뉴런 C가 활성화되어 있고 반대의 경우도 마찬가지입니다.\n",
    "\n",
    "이러한 네트워크를 결합하여 복잡한 논리 표현식을 계산하는 방법을 쉽게 상상할 수 있습니다 (이 장의 끝 부분에있는 실습 참조).\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "Perceptron은 Frank Rosenblatt가 1957 년에 발명 한 가장 단순한 ANN 아키텍처 중 하나입니다. 선형 임계 값 단위 (LTU) 라 불리는 약간 다른 인공 뉴런 (그림 10-4 참조)을 기반으로합니다 : 입력 및 출력이 이제는 숫자입니다 (바이너리 온 / 오프 값 대신). 각 입력 연결은 가중치와 연결됩니다 . LTU는 입력의 가중치 합을 계산 한 다음 그 합에 계단 함수를 적용하여 결과를 출력합니다. hw (x) = step (z) = step (wT · x).\n",
    "\n",
    "![Image](figures/4.png)\n",
    "\n",
    "퍼셉트론에서 사용되는 가장 일반적인 단계 함수는 Heaviside 스텝 함수입니다 (식 10-1 참조). 때때로 sign 함수가 대신 사용됩니다.\n",
    "\n",
    "![Image](figures/5.png)\n",
    "\n",
    "단일 선형 이진 분류에 단일 LTU를 사용할 수 있습니다. 입력의 선형 조합을 계산하고 결과가 임계 값을 초과하는 경우 양수 클래스를 출력하거나 그렇지 않으면 (Logistic Regression 분류기 또는 선형 SVM과 같은) 음수 클래스를 출력합니다. 예를 들어, 단일 LTU를 사용하여 꽃잎의 길이와 너비를 기반으로 홍채 꽃을 분류 할 수 있습니다 (이전 장에서했던 것처럼 추가 바이어스 기능 x0 = 1 추가). LTU를 훈련한다는 것은 w0, w1 및 w2에 대한 올바른 값을 찾는 것을 의미합니다 (훈련 알고리즘은 곧 논의됩니다).\n",
    "\n",
    "퍼셉트론은 LTU의 단일 층으로 구성되며, 모든 입력에 연결된 각 뉴런이 있습니다. 이러한 연결은 종종 입력 뉴런이라고하는 특수 통과 뉴런을 사용하여 표현됩니다. 입력 된 모든 입력을 출력합니다. 또한 추가 바이어스 특성이 일반적으로 추가됩니다 (x0 = 1). 이 바이어스 특성은 일반적으로 항상 1을 출력하는 바이어스 뉴런이라는 특별한 유형의 뉴런을 사용하여 표현됩니다.\n",
    "\n",
    "그림 10-5에는 두 개의 입력과 세 개의 출력을 갖는 퍼셉트론이 표시되어있다. 이 Perceptron은 인스턴스를 동시에 세 가지 다른 이진 클래스로 분류 할 수 있으므로 다중 출력 분류 자입니다.\n",
    "\n",
    "![Image](figures/6.png)\n",
    "\n",
    "\n",
    "퍼셉트론은 어떻게 훈련 받았습니까? Frank Rosenblatt가 제안한 Perceptron 교육 알고리즘은 Hebb의 규칙에 크게 영향을 받았습니다. 도널드 헤브 (Donald Hebb)는 1949 년에 출판 된 e Behavior of Organisation에서 생물학적 뉴런이 종종 다른 뉴런을 유발할 때이 두 뉴런의 연결이 강하게 성장한다고 제안했다. 이 아이디어는 Siegrid Löwel에 의해 다음과 같이이 캐치 프레이즈로 요약되었습니다. \"함께 타는 세포는 서로 연결됩니다.\"이 규칙은 나중에 Hebb의 규칙 (또는 Hebbian 학습)으로 알려지게되었습니다. 즉, 동일한 출력을 가질 때마다 두 뉴런 간의 연결 가중치가 증가합니다. 퍼셉트론은 네트워크에 의해 발생한 에러를 고려한이 규칙을 사용하여 학습됩니다. 잘못된 출력으로 연결되는 연결을 보강하지 않습니다. 보다 구체적으로 말하면, 퍼셉트론은 한 번에 하나의 교육 인스턴스로 공급되며, 각 인스턴스에 대해 예측을 수행합니다. 잘못된 예측을 산출 한 모든 출력 뉴런에 대해 정확한 예측에 기여한 입력으로부터 연결 가중치를 강화합니다. 규칙은 식 10-2와 같습니다.\n",
    "\n",
    "![Image](figures/7.png)\n",
    "\n",
    "각 출력 뉴런의 결정 경계는 선형이므로 Perceptron은 복잡한 패턴을 학습 할 수 없습니다. (Logistic Regression 분류기와 유사 함) 그러나 트레이닝 인스턴스가 선형 적으로 분리 가능한 경우 Rosenblatt는이 알고리즘이 솔루션에 수렴한다는 것을 입증했습니다.이를 퍼셉트론 컨버전스 정리라고합니다.\n",
    "Scikit-Learn은 단일 LTU 네트워크를 구현하는 Perceptron 클래스를 제공합니다. 예를 들어, 홍채 데이터 세트 (예 : 4 장에서 소개 함)에서 예상대로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa?\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron 학습 알고리즘이 Stochastic Gradient Descent와 매우 유사하다는 것을 알고있을 것입니다. 실제로 Scikit-Learn의 Perceptron 클래스는 다음과 같은 하이퍼 매개 변수와 함께 SGDClassifier를 사용하는 것과 같습니다.\n",
    "\n",
    "loss = \"perceptron\", learning_rate = \"constant\", eta0 = 1 (학습 속도) 및 penalty = None.\n",
    "\n",
    "로지스틱 회귀 분류기와 달리, 퍼셉트론은 클래스 확률을 출력하지 않습니다. 오히려 하드 임계 값을 기반으로 예측을합니다. 이것은 Perceptrons에 대한 Logistic Regression을 선호하는 좋은 이유 중 하나입니다.\n",
    "\n",
    "Perceptrons라는 제목의 1969 년 논문에서 Marvin Minsky와 Seymour Papert는 Perceptrons의 심각한 약점, 특히 사소한 문제 (예 : Exclusive OR (XOR) 분류 문제)를 해결할 수 없다는 사실을 강조했다. 그림 10-6). 물론 이것은 로지스틱 회귀 분류기와 같은 다른 선형 분류 모델에서도 마찬가지이지만 연구원은 퍼셉트론에서 더 많은 것을 기대했으며 실망감이 컸다. 결과적으로 많은 연구자들이 연결주의를 완전히 포기했다 (즉 연구 논리 네트워크, 문제 해결 및 검색과 같은 상위 수준의 문제에 유리합니다.\n",
    "\n",
    "그러나 여러 가지 퍼셉트론을 스태킹하면 퍼셉트론의 한계 중 일부를 제거 할 수 있습니다. 결과 ANN은 MLP (Multi-Layer Perceptron)라고합니다. 특히, MLP는 XOR 문제를 해결할 수 있습니다. 그림 10-6의 오른쪽에 표시된 MLP의 출력을 입력의 각 조합에 대해 입력 (0, 0) 또는 (1, 1) 네트워크는 0을 출력하고 입력 (0, 1) 또는 (1, 0)을 출력하면 1을 출력합니다.\n",
    "\n",
    "![Image](figures/8.png)\n",
    "\n",
    "## Multi-Layer Perceptron and Backpropagation\n",
    "\n",
    "MLP는 하나의 (패스 스루) 입력 레이어, 숨겨진 레이어라고하는 하나 이상의 LTU 레이어 및 출력 레이어라고하는 LTU의 최종 레이어로 구성됩니다 (그림 10-7 참조). 출력 레이어를 제외한 모든 레이어는 바이어스 뉴런을 포함하며 다음 레이어에 완전히 연결됩니다. ANN에 두 개 이상의 숨겨진 레이어가있는 경우이를 DNN (Deep Neural Network)이라고합니다.\n",
    "\n",
    "![Image](figures/9.png)\n",
    "\n",
    "수년 동안 연구자들은 성공하지 못하고 MLP를 훈련시키는 방법을 찾는데 어려움을 겪었습니다. 그러나 1986 년 D. E. Rumelhart et al. backpropagation training 알고리즘을 소개하는 획기적인 기사를 발표했습니다 .9 오늘은 reverse-mode autodiff (Gradient Descent는 4 장에서 소개되었고 autodiff는 9 장에서 논의되었습니다)를 사용하여 Gradient Descent로 설명 할 것입니다.\n",
    "\n",
    "각 교육 인스턴스에 대해 알고리즘은 네트워크에 피드를 제공하고 각 연속 레이어의 모든 뉴런 출력을 계산합니다 (예상을 만들 때와 마찬가지로 순방향 전달입니다). 그런 다음 네트워크의 출력 오류 (즉, 원하는 출력과 네트워크의 실제 출력 간의 차이)를 측정하고 마지막 숨겨진 계층의 각 뉴런이 각 출력 뉴런의 오류에 기여한 정도를 계산합니다. 그런 다음 알고리즘은 입력 레이어에 도달 할 때까지 이전 숨겨진 레이어의 각 뉴런에서 발생한 오류 기여도를 측정합니다. 이 역 통과는 네트워크에서 오류 그라디언트를 역방향으로 전파함으로써 네트워크의 모든 연결 가중치에 대한 오류 그라데이션을 효율적으로 측정합니다 (따라서 알고리즘의 이름). 부록 D에서 역 모드 자동 디코딩 알고리즘을 체크 아웃하면, 역 전파의 순방향 및 역방향 패스가 단순히 역방향 모드 자동 확산을 수행한다는 것을 알 수 있습니다. 역 전파 알고리즘의 마지막 단계는 이전 측정 된 오류 기울기를 사용하여 네트워크의 모든 연결 가중치에 대한 그라데이션 하강 단계입니다.\n",
    "\n",
    "각 학습 인스턴스에 대해 역 전파 알고리즘은 먼저 예측 (순방향 통과)을 수행하고 오류를 측정 한 다음 각 계층을 역순으로 통과하여 각 연결 (역 통과)에서 오류 기여도를 측정하고 마지막으로 오류를 줄이기 위해 연결 가중치를 약간 조정합니다 (Gradient Descent 단계).\n",
    "\n",
    "이 알고리즘이 제대로 작동하려면 작성자가 MLP의 아키텍처를 크게 변경했습니다. 단계 함수를 로지스틱 함수 σ (z) = 1 / (1 + exp (-z))로 대체했습니다. 계단 함수에는 평면 세그먼트 만 포함되어 있기 때문에 필수적 이었으므로 작업 할 그라디언트가 없습니다 (그라디언트 하강은 평평한 표면에서 움직일 수 없음). 반면에 로지스틱 함수는 모든 경우에 잘 정의 된 0이 아닌 미분을 가지며 그라디언트 하강은 모든 단계에서 약간의 진전을 이루기위한 것입니다. 역 발상 알고리즘은 물류 기능 대신 다른 활성화 기능과 함께 사용될 수 있습니다. 두 가지 다른 인기있는 정품 인증 기능은 다음과 같습니다.\n",
    "\n",
    "- The Hyperbolic tangent function tanh (z) = 2σ (2z) - 1\n",
    "> 물류 기능과 마찬가지로 S 자형이고 연속적이며 차별화가 가능하지만 출력 값의 범위는 물류 기능의 경우 0에서 1 대신에 -1에서 1까지이며 각 레이어의 출력을 만드는 경향이 있습니다 훈련이 시작될 때 어느 정도 정규화 (즉, 약 0도 중앙)됩니다. 이렇게하면 수렴 속도가 빨라집니다.\n",
    "\n",
    "- The ReLU function (9 장에서 소개)\n",
    "> ReLU (z) = max (0, z). 그것은 연속적이지만 불행하게도 z = 0에서 구별 할 수 없습니다 (기울기가 갑자기 변하기 때문에 그라디언트 하강이 바운스 될 수 있습니다). 그러나 실제로는 매우 잘 작동하며 계산 속도가 빠르다는 이점이 있습니다. 가장 중요한 점은, 최대 출력 값을 가지지 않는다는 사실은 그라디언트 하강 동안 일부 문제를 줄이는 데 도움이된다는 것을 의미합니다 (11 장에서 다시 설명 할 것입니다).\n",
    "\n",
    "![Image](figures/10.png)\n",
    "\n",
    "MLP는 종종 분류에 사용되며 각 출력은 다른 바이너리 클래스 (예 : 스팸 / 햄, 긴급 / 비 긴급 등)에 해당합니다. 클래스가 배타적 인 경우 (예 : 숫자 이미지 분류의 경우 클래스 0-9) 출력 레이어는 일반적으로 개별 활성화 함수를 공유 된 최대 - 최대 함수로 대체하여 수정됩니다 (그림 10-9 참조). softmax 함수는 3 장에서 소개되었다. 각 뉴런의 출력은 해당 클래스의 추정 확률에 해당한다. 신호는 한 방향 (입력에서 출력까지)으로 만 흐릅니다. 따라서이 아키텍처는 FNN (Feedforward Neural Network)의 예입니다.\n",
    "\n",
    "![Image](figures/11.png)\n",
    "\n",
    "## Training a DNN Using Plain TensorFlow\n",
    "\n",
    "네트워크 아키텍처를보다 잘 제어하려면 TensorFlow의 하위 레벨 Python API (9 장에서 소개 함)를 사용하는 것이 좋습니다. 이 섹션에서는이 API를 사용하기 전과 동일한 모델을 구축하고 Mini-batch Gradient Descent를 구현하여 MNIST 데이터 세트에서이를 학습합니다. 첫 번째 단계는 TensorFlow 그래프를 작성하는 구축 단계입니다. 두 번째 단계는 실행 단계입니다. 여기서 실제로 그래프를 실행하여 모델을 교육합니다.\n",
    "\n",
    "### Construction Phase\n",
    "\n",
    "시작하자. 먼저 tensorflow 라이브러리를 가져와야합니다. 그런 다음 입력 및 출력 수를 지정하고 각 계층에서 숨겨진 뉴런 수를 설정해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 9 장에서했던 것처럼 자리 표시 자 노드를 사용하여 교육 데이터와 대상을 나타낼 수 있습니다. X의 모양은 부분적으로 만 정의됩니다. 첫 번째 차원의 인스턴스와 두 번째 차원의 차원을 가진 2D 텐서 (즉, 행렬)가 될 것이고, 피쳐의 수는 28 x 28 (픽셀 당 하나의 피쳐)이 될 것이라는 것을 알고 있습니다. , 그러나 우리는 각 훈련 배치에 얼마나 많은 인스턴스가 포함 될지 아직 모른다. 그래서 X의 모양은 (None, n_inputs)입니다. 마찬가지로 y는 인스턴스 당 하나의 엔트리를 가진 1D 텐서가되지만이 시점에서 트레이닝 배치의 크기를 알 수 없기 때문에 모양은 (None)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 실제 신경망을 만들어 보겠습니다. 자리 표시 자 X는 입력 레이어로 작동합니다. 실행 단계에서 한 번에 하나의 교육 배치로 대체됩니다 (교육 배치의 모든 인스턴스는 신경 네트워크에 의해 동시에 처리됩니다). 이제 두 개의 숨겨진 레이어와 출력 레이어를 만들어야합니다. 두 개의 숨겨진 레이어는 거의 동일합니다. 연결되어있는 입력과 포함 된 뉴런의 수에 따라 다릅니다. 출력 계층도 매우 유사하지만 ReLU 활성화 기능 대신 softmax 활성화 기능을 사용합니다. 한 번에 하나의 레이어를 만드는 데 사용할 neuron_layer () 함수를 만듭니다. 입력, 뉴런 수, 활성화 함수 및 레이어 이름을 지정하는 매개 변수가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드를 한 줄씩 살펴 보겠습니다.\n",
    "\n",
    "> 1. 먼저 레이어의 이름을 사용하여 이름 범위를 만듭니다.이 레이어에는이 뉴런 레이어에 대한 모든 계산 노드가 포함됩니다. 이것은 선택 사항이지만 노드가 잘 구성되어 있으면 그래프가 TensorBoard에서 훨씬 더 멋지게 보입니다.\n",
    "2. 다음으로, 입력 행렬의 모양을 찾고 두 번째 차원의 크기를 가져 와서 입력 수를 얻습니다 (첫 번째 차원은 인스턴스 용입니다).\n",
    "3. 다음 세 행은 가중치 행렬을 보유 할 W 변수를 작성합니다. 각 입력과 각 뉴런 사이의 모든 연결 가중치를 포함하는 2D 텐서가됩니다. 따라서 그 모양은 (n_inputs, n_neurons)가됩니다. 표준 편차가 2 / ninputs 인 truncated10 normal (Gaussian) 분포를 사용하여 무작위로 초기화됩니다. 이 특정 표준 편차를 사용하면 알고리즘이 훨씬 빨리 수렴하는 데 도움이됩니다 (11 장에서 더 자세히 논의 할 예정이며 효율성에 엄청난 영향을 미친 신경 네트워크에 대한 작은 조정 중 하나입니다). Gradient Descent 알고리즘이 깨질 수없는 대칭을 피하기 위해 모든 숨겨진 레이어에 대해 연결 가중치를 임의로 초기화하는 것이 중요합니다.\n",
    "4. 다음 줄은 바이어스에 대한 b 변수를 만듭니다.이 변수는 뉴런 당 하나의 바이어스 매개 변수를 사용하여 0으로 초기화됩니다 (이 경우 대칭 문제 없음).\n",
    "5. 그런 다음 z = X / W + b를 계산하기위한 부분 그래프를 만듭니다. 이 벡터화 된 구현은 한 번에 배치의 모든 인스턴스에 대해 입력의 가중치 합계와 레이어의 각 뉴런에 대한 바이어스 조건을 효율적으로 계산합니다.\n",
    "6. 마지막으로 활성화 매개 변수가 \"relu\"로 설정된 경우 코드에서 relu (z) (즉, max (0, z))를 반환하거나 아니면 z를 반환합니다.\n",
    "\n",
    "이제는 뉴런 레이어를 만드는 멋진 기능이 있습니다. 깊은 신경 네트워크를 만드는 데 사용합시다! 첫 번째 숨겨진 계층은 X를 입력으로 사용합니다. 두 번째는 첫 번째 숨겨진 레이어의 출력을 입력으로 사용합니다. 마지막으로 출력 레이어는 두 번째 숨겨진 레이어의 출력을 입력으로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "    logits = neuron_layer(hidden2, n_outputs, \"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 한번 우리는 명확성을 위해 이름 범위를 사용했습니다. 또한 logits는 softmax 활성화 함수를 거치기 전에 신경망의 출력입니다. 최적화를 위해 나중에 softmax 계산을 처리 할 것입니다.\n",
    "\n",
    "예상대로 TensorFlow에는 표준 신경망 레이어를 만드는 데 유용한 많은 기능이 제공되므로 방금 수행 한 것처럼 자신의 neuron_layer () 함수를 정의 할 필요가없는 경우가 종종 있습니다. 예를 들어, TensorFlow의 fully_connec ted () 함수는 완전히 연결된 레이어를 만듭니다. 여기서 모든 입력은 레이어의 모든 뉴런에 연결됩니다. 적절한 초기화 전략을 사용하여 가중치 및 바이어스 변수를 작성하고 기본적으로 ReLU 활성화 기능을 사용합니다 (activation_fn 인수를 사용하여 변경할 수 있음). 11 장에서 볼 수 있듯이 정규화 및 정규화 매개 변수도 지원합니다. neu ron_layer () 함수 대신에 fully_connected () 함수를 사용하는 앞의 코드를 조정 해보자. 간단히 함수를 가져 와서 dnn 생성 섹션을 다음 코드로 바꿉니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\", activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 신경망 모델을 사용할 준비가되었습니다. 우리는이를 훈련시키는 데 사용할 비용 함수를 정의해야합니다. 우리가 4 장에서 Softmax Regression을 위해했던 것처럼, 우리는 교차 엔트로피를 사용할 것입니다. 앞서 논의한 것처럼 교차 엔트로피는 목표 클래스에 대한 낮은 확률을 추정하는 모델에 불이익을줍니다. TensorFlow는 교차 엔트로피를 계산하는 몇 가지 기능을 제공합니다. 우리는 sparse_soft max_cross_entropy_with_logits ()를 사용할 것입니다 : \"logits\"(즉, softmax 활성화 함수를 거치기 전에 네트워크의 출력)를 기반으로 크로스 엔트로피를 계산합니다. 0에서부터 클래스 수 - 1 (우리의 경우 0에서 9). 이것은 각 인스턴스에 대해 교차 엔트로피를 포함하는 1D 텐서를 제공합니다. 그런 다음 TensorFlow의 reduce_mean () 함수를 사용하여 모든 인스턴스에 대한 평균 교차 엔트로피를 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse_softmax_cross_entropy_with_logits () 함수는 softmax 활성화 함수를 적용한 다음 교차 엔트로피를 계산하는 것과 같지만 더 효율적이며 0 인 로그와 같은 모서리 사례를 올바르게 처리합니다. 따라서 우리는 softmax 활성화 기능이 빠릅니다. 또한 softmax_cross_entropy_with_logits ()라는 또 다른 함수가 있습니다.이 함수는 0에서부터 클래스 수 -1까지의 int 대신에 하나의 핫 벡터 형식으로 레이블을 사용합니다.\n",
    "\n",
    "우리는 신경망 모델을 가지고 있고, 우리는 비용 함수를 가지고 있으며, 이제 우리는 비용 함수를 최소화하기 위해 모델 매개 변수를 조정할 GradientDescentOptimizer를 정의해야합니다. 새로운 것은 없다; 우리가 9 장에서했던 것과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구축 단계의 마지막 중요한 단계는 모델을 평가하는 방법을 지정하는 것입니다. 우리는 단순히 성능 척도로서 정확도를 사용합니다. 첫째, 각 인스턴스에 대해 가장 높은 logit이 목표 클래스에 해당하는지 여부를 확인하여 신경망의 예측이 올바른지 확인합니다. 이를 위해 in_top_k () 함수를 사용할 수 있습니다. 부울 값으로 가득 찬 1D 텐서를 반환하므로이 부울을 부동 소수점 형으로 캐스팅 한 다음 평균을 계산해야합니다. 이것은 우리에게 네트워크의 전반적인 정확성을 줄 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평소처럼 모든 변수를 초기화하기 위해 노드를 만들어야하며 훈련 된 모델 매개 변수를 디스크에 저장하기 위해 Saver도 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "휴! 이것으로 건설 단계가 끝납니다. 이것은 40 줄 미만의 코드 였지만 입력 및 대상에 대한 자리 표시자를 만들었고 신경 층을 만드는 함수를 만들고 DNN을 만드는 데 사용했으며 비용 함수를 정의했습니다. 최적화 도구를 만들고 마지막으로 성능 측정 값을 정의했습니다. 이제 실행 단계로 넘어갑니다.\n",
    "\n",
    "### Execution Phase\n",
    "\n",
    "이 부분은 훨씬 더 짧고 간단합니다. 우선, MNIST를로드 해 봅시다. 이전 장에서했던 것처럼 Scikit- Learn를 사용할 수 있지만 TensorFlow는 데이터를 가\n",
    "져 와서 크기를 조정하고 (0에서 1 사이) 자체를 도우며 셔플 링하고 하나의 미니 배치를로드하는 간단한 함수를 제공합니다. 시각. 그럼 대신 사용 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.9 Test accuracy: 0.902\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9215\n",
      "2 Train accuracy: 0.92 Test accuracy: 0.929\n",
      "3 Train accuracy: 0.94 Test accuracy: 0.9348\n",
      "4 Train accuracy: 0.94 Test accuracy: 0.9422\n",
      "5 Train accuracy: 0.9 Test accuracy: 0.9466\n",
      "6 Train accuracy: 0.94 Test accuracy: 0.9496\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9534\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.9552\n",
      "9 Train accuracy: 0.96 Test accuracy: 0.9577\n",
      "10 Train accuracy: 0.96 Test accuracy: 0.9607\n",
      "11 Train accuracy: 0.94 Test accuracy: 0.9616\n",
      "12 Train accuracy: 0.98 Test accuracy: 0.964\n",
      "13 Train accuracy: 0.98 Test accuracy: 0.9648\n",
      "14 Train accuracy: 0.98 Test accuracy: 0.9668\n",
      "15 Train accuracy: 0.98 Test accuracy: 0.9671\n",
      "16 Train accuracy: 1.0 Test accuracy: 0.9674\n",
      "17 Train accuracy: 1.0 Test accuracy: 0.9697\n",
      "18 Train accuracy: 1.0 Test accuracy: 0.97\n",
      "19 Train accuracy: 0.98 Test accuracy: 0.9709\n",
      "20 Train accuracy: 0.98 Test accuracy: 0.971\n",
      "21 Train accuracy: 0.98 Test accuracy: 0.9728\n",
      "22 Train accuracy: 0.98 Test accuracy: 0.972\n",
      "23 Train accuracy: 1.0 Test accuracy: 0.973\n",
      "24 Train accuracy: 0.98 Test accuracy: 0.9731\n",
      "25 Train accuracy: 1.0 Test accuracy: 0.9734\n",
      "26 Train accuracy: 0.98 Test accuracy: 0.9744\n",
      "27 Train accuracy: 1.0 Test accuracy: 0.975\n",
      "28 Train accuracy: 1.0 Test accuracy: 0.9751\n",
      "29 Train accuracy: 1.0 Test accuracy: 0.975\n",
      "30 Train accuracy: 0.96 Test accuracy: 0.9754\n",
      "31 Train accuracy: 0.94 Test accuracy: 0.9741\n",
      "32 Train accuracy: 1.0 Test accuracy: 0.9757\n",
      "33 Train accuracy: 1.0 Test accuracy: 0.9765\n",
      "34 Train accuracy: 0.98 Test accuracy: 0.9763\n",
      "35 Train accuracy: 1.0 Test accuracy: 0.9764\n",
      "36 Train accuracy: 1.0 Test accuracy: 0.9764\n",
      "37 Train accuracy: 1.0 Test accuracy: 0.9776\n",
      "38 Train accuracy: 0.98 Test accuracy: 0.9775\n",
      "39 Train accuracy: 1.0 Test accuracy: 0.978\n",
      "40 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "41 Train accuracy: 1.0 Test accuracy: 0.9774\n",
      "42 Train accuracy: 1.0 Test accuracy: 0.9776\n",
      "43 Train accuracy: 1.0 Test accuracy: 0.9779\n",
      "44 Train accuracy: 0.96 Test accuracy: 0.9779\n",
      "45 Train accuracy: 0.98 Test accuracy: 0.9777\n",
      "46 Train accuracy: 0.98 Test accuracy: 0.9775\n",
      "47 Train accuracy: 1.0 Test accuracy: 0.9787\n",
      "48 Train accuracy: 1.0 Test accuracy: 0.979\n",
      "49 Train accuracy: 1.0 Test accuracy: 0.978\n",
      "50 Train accuracy: 1.0 Test accuracy: 0.9782\n",
      "51 Train accuracy: 0.98 Test accuracy: 0.9786\n",
      "52 Train accuracy: 0.98 Test accuracy: 0.9786\n",
      "53 Train accuracy: 1.0 Test accuracy: 0.9785\n",
      "54 Train accuracy: 1.0 Test accuracy: 0.9782\n",
      "55 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "56 Train accuracy: 1.0 Test accuracy: 0.979\n",
      "57 Train accuracy: 1.0 Test accuracy: 0.9785\n",
      "58 Train accuracy: 1.0 Test accuracy: 0.9785\n",
      "59 Train accuracy: 1.0 Test accuracy: 0.9784\n",
      "60 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "61 Train accuracy: 1.0 Test accuracy: 0.9791\n",
      "62 Train accuracy: 1.0 Test accuracy: 0.9789\n",
      "63 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "64 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "65 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "66 Train accuracy: 1.0 Test accuracy: 0.9789\n",
      "67 Train accuracy: 0.98 Test accuracy: 0.9801\n",
      "68 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "69 Train accuracy: 1.0 Test accuracy: 0.979\n",
      "70 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "71 Train accuracy: 1.0 Test accuracy: 0.9791\n",
      "72 Train accuracy: 1.0 Test accuracy: 0.9792\n",
      "73 Train accuracy: 1.0 Test accuracy: 0.9788\n",
      "74 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "75 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "76 Train accuracy: 1.0 Test accuracy: 0.9787\n",
      "77 Train accuracy: 1.0 Test accuracy: 0.9784\n",
      "78 Train accuracy: 1.0 Test accuracy: 0.9795\n",
      "79 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "80 Train accuracy: 1.0 Test accuracy: 0.9788\n",
      "81 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "82 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "83 Train accuracy: 1.0 Test accuracy: 0.9795\n",
      "84 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "85 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "86 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "87 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "88 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "89 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "90 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "91 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "92 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "93 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "94 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "95 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "96 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "97 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "98 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "99 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "100 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "101 Train accuracy: 1.0 Test accuracy: 0.9795\n",
      "102 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "103 Train accuracy: 1.0 Test accuracy: 0.979\n",
      "104 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "105 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "106 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "107 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "108 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "109 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "110 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "111 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "112 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "113 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "114 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "115 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "116 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "117 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "118 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "119 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "120 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "121 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "122 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "123 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "124 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "125 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "126 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "127 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "128 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "129 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "130 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "131 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "132 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "133 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "134 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "135 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "136 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "137 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "138 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "139 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "140 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "141 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "142 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "143 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "144 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "145 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "146 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "147 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "148 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "149 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "150 Train accuracy: 1.0 Test accuracy: 0.9796\n",
      "151 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "152 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "153 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "154 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "155 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "156 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "157 Train accuracy: 1.0 Test accuracy: 0.9795\n",
      "158 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "159 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "160 Train accuracy: 1.0 Test accuracy: 0.9806\n",
      "161 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "162 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "163 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "164 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "165 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "166 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "167 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "168 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "169 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "170 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "171 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "172 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "173 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "174 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "175 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "176 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "177 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "178 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "179 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "180 Train accuracy: 1.0 Test accuracy: 0.9799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "182 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "183 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "184 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "185 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "186 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "187 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "188 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "189 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "190 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "191 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "192 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "193 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "194 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "195 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "196 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "197 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "198 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "199 Train accuracy: 1.0 Test accuracy: 0.9805\n",
      "200 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "201 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "202 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "203 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "204 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "205 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "206 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "207 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "208 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "209 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "210 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "211 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "212 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "213 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "214 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "215 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "216 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "217 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "218 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "219 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "220 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "221 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "222 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "223 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "224 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "225 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "226 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "227 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "228 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "229 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "230 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "231 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "232 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "233 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "234 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "235 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "236 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "237 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "238 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "239 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "240 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "241 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "242 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "243 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "244 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "245 Train accuracy: 1.0 Test accuracy: 0.9805\n",
      "246 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "247 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "248 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "249 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "250 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "251 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "252 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "253 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "254 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "255 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "256 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "257 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "258 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "259 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "260 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "261 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "262 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "263 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "264 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "265 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "266 Train accuracy: 1.0 Test accuracy: 0.9805\n",
      "267 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "268 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "269 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "270 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "271 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "272 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "273 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "274 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "275 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "276 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "277 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "278 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "279 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "280 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "281 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "282 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "283 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "284 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "285 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "286 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "287 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "288 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "289 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "290 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "291 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "292 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "293 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "294 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "295 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "296 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "297 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "298 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "299 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "300 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "301 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "302 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "303 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "304 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "305 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "306 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "307 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "308 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "309 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "310 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "311 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "312 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "313 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "314 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "315 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "316 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "317 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "318 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "319 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "320 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "321 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "322 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "323 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "324 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "325 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "326 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "327 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "328 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "329 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "330 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "331 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "332 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "333 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "334 Train accuracy: 1.0 Test accuracy: 0.9806\n",
      "335 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "336 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "337 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "338 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "339 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "340 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "341 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "342 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "343 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "344 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "345 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "346 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "347 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "348 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "349 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "350 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "351 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "352 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "353 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "354 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "355 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "356 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "357 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "358 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "359 Train accuracy: 1.0 Test accuracy: 0.9801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "361 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "362 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "363 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "364 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "365 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "366 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "367 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "368 Train accuracy: 1.0 Test accuracy: 0.9798\n",
      "369 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "370 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "371 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "372 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "373 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "374 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "375 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "376 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "377 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "378 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "379 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "380 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "381 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "382 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "383 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "384 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "385 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "386 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "387 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "388 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "389 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "390 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "391 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "392 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "393 Train accuracy: 1.0 Test accuracy: 0.9805\n",
      "394 Train accuracy: 1.0 Test accuracy: 0.9801\n",
      "395 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "396 Train accuracy: 1.0 Test accuracy: 0.9802\n",
      "397 Train accuracy: 1.0 Test accuracy: 0.9803\n",
      "398 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "399 Train accuracy: 1.0 Test accuracy: 0.9803\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 TensorFlow 세션을 열고 모든 변수를 초기화하는 init 노드를 실행합니다. 그런 다음 기본 교육 루프를 실행합니다. 각 신기원에서 코드는 교육 세트 크기에 해당하는 여러 가지 미니 배치를 반복합니다. 각 미니 배치는 next_batch () 메소드를 통해 가져오고 코드는 현재 트레이닝 작업을 실행하고 현재 미니 배치 입력 데이터와 대상을 제공합니다. 다음으로, 각 에포크의 끝에서, 코드는 마지막 미니 배치와 풀 트레이닝 세트에서 모델을 평가하고 결과를 인쇄합니다. 마지막으로 모델 매개 변수가 디스크에 저장됩니다.\n",
    "\n",
    "### Using the Neural Network\n",
    "\n",
    "신경 네트워크가 훈련되었으므로 예측을 위해 사용할 수 있습니다. 그렇게하기 위해 동일한 구성 단계를 재사용 할 수 있지만 다음과 같이 실행 단계를 변경하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "#     X_new_scaled = [...] # some new images (scaled from 0 to 1)\n",
    "#     Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "#     y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 코드는 디스크에서 모델 매개 변수를로드합니다. 그런 다음 분류 할 새 이미지를로드합니다. 트레이닝 데이터와 동일한 피쳐 스케일링을 적용해야합니다 (이 경우, 0에서 1로 스케일). 그런 다음 코드는 logits 노드를 평가합니다. 모든 예상 클래스 확률을 알고 싶다면 softmax () 함수를 logits에 적용해야하지만 클래스를 예측하려는 경우 가장 높은 logit 값을 가진 클래스를 선택할 수 있습니다 ( argmax () 함수는 트릭을 수행한다.\n",
    "\n",
    "## Fine-Tuning Neural Network Hyperparameters\n",
    "\n",
    "신경 네트워크의 유연성 또한 주요 단점 중 하나입니다. 조정할 수있는 많은 하이퍼 파라미터가 있습니다. 상상할 수있는 네트워크 토폴로지 (뉴런이 어떻게 상호 연결되는지)를 사용할 수있을뿐만 아니라 간단한 MLP에서도 레이어 수, 레이어 당 뉴런 수, 각 레이어에서 사용할 활성화 함수 유형, 가중치를 변경할 수 있습니다 초기화 로직 등을 제공합니다. 하이퍼 매개 변수의 조합이 작업에 가장 적합한 지 어떻게 알 수 있습니까?\n",
    "\n",
    "물론 이전 장에서와 같이 교차 분석을 사용하여 그리드 검색을 사용하여 적절한 하이퍼 매개 변수를 찾을 수 있지만 튜닝 할 하이퍼 매개 변수가 많기 때문에 대규모 데이터 집합에서 신경 네트워크를 학습하는 데는 많은 시간이 걸리기 때문에 시간이 지나면 합리적인 시간 내에 하이퍼 매개 변수 공간의 아주 작은 부분 만 탐색 할 수 있습니다. 2 장에서 설명한 것처럼 무작위 검색을 사용하는 것이 훨씬 더 좋습니다. 또 다른 옵션은 Ospar와 같은 도구를 사용하는 것입니다.이 도구는 더 복잡한 매개 변수를 신속하게 찾을 수 있도록보다 복잡한 알고리즘을 구현합니다.\n",
    "\n",
    "각 하이퍼 매개 변수에 대해 합리적인 값이 무엇인지 생각하면 검색 공간을 제한 할 수 있습니다. 숨겨진 레이어의 수부터 시작해 보겠습니다.\n",
    "\n",
    "### Number of Hidden Layers\n",
    "\n",
    "많은 문제들에 대해, 당신은 단지 하나의 숨겨진 레이어로 시작할 수 있으며, 당신은 합리적인 결과를 얻을 것입니다. 실제로 하나의 숨겨진 레이어가있는 MLP는 충분한 뉴런을 제공한다면 가장 복잡한 기능조차도 모델링 할 수 있습니다. 오랜 시간 동안, 이러한 사실은 연구원들이 더 깊은 신경 네트워크를 조사 할 필요가 없다고 확신 시켰습니다. 훈련하기가 훨씬 더 빨리 만들기, 그들은 얕은 그물에 비해 기하 급수적으로 적은 수의 뉴런을 사용하여 복잡한 기능을 모델링 할 수 있습니다 :하지만 그들은 깊은 네트워크가 얕은 것보다 훨씬 더 높은 매개 변수 전자의 결핍증을 가지고 있다는 사실을 간과.\n",
    "\n",
    "이유를 이해하려면 드로잉 소프트웨어를 사용하여 포리스트를 그리도록 요청 받았지만 복사 / 붙여 넣기를 사용하는 것은 금지되어 있습니다. 각 트리를 개별적으로 그려야하고, 분기마다 분기하고 리프마다 리프를 그려야합니다. 하나의 잎을 그릴 수 있다면 그것을 복사하여 붙여 넣기를하고 그 지점을 복사 / 붙여 넣기하여 나무를 만들고 마지막으로이 나무를 복사하여 붙여 넣어 숲을 만들면 곧 마칠 수 있습니다. 실세계 데이터는 종종 계층 적 방식으로 구성되어 DNNs 자동으로이 사실을 활용 : 낮은 숨겨진 레이어 모델 낮은 수준의 구조 (예를 들어, 라인 다양한 모양과 방향의 세그먼트), 중간 숨겨진 층이 낮은 수준의 구조를 결합 중간 레벨 구조 (예 : 사각형, 원형)를 모델링하고 최상위 숨겨진 레이어와 출력 레이어는 이러한 중간 구조를 결합하여 상위 수준 구조 (예 : 얼굴)를 모델링합니다.\n",
    "\n",
    "이 계층 적 아키텍처는 DNN이 좋은 솔루션으로 빠르게 수렴 할 수있을뿐 아니라 새로운 데이터 세트로 일반화 할 수있는 능력을 향상시킵니다. 이미 사진에서 얼굴을 인식 할 수있는 모델을 훈련 한, 당신은 지금 헤어 스타일을 인식 할 수있는 새로운 신경 네트워크를 양성하려는 경우 예를 들어, 첫 번째 네트워크의 하위 계층을 재사용하여 교육을 킥 스타트 할 수 있습니다. 대신 무작위로 새로운 신경망의 처음 몇 층의 무게와 편견을 초기화, 당신은 첫 번째 네트워크의 하위 계층의 무게와 편견의 값을 초기화 할 수 있습니다. 이 방법으로 네트워크는 대부분의 그림에서 발생하는 모든 저수준 구조를 처음부터 배우지 않아도됩니다. 상위 수준 구조 (예 : 헤어 스타일) 만 학습하면됩니다.\n",
    "\n",
    "요약하면, 많은 문제에 대해 하나 또는 두 개의 숨겨진 레이어로 시작할 수 있습니다. (예를 들어, 몇 백 개의 뉴런이있는 하나의 숨겨진 레이어를 사용하여 MNIST 데이터 세트에서 97 % 이상의 정확도를 쉽게 얻을 수 있습니다. 대략 동일한 양의 뉴런을 가진 2 개의 숨겨진 레이어를 사용하여 대략 98 %의 정확도, 대략 동일한 양의 교육 시간). 좀 더 복잡한 문제의 경우, 교육 세트를 과도하게 시작할 때까지 숨겨진 레이어의 수를 점차적으로 늘릴 수 있습니다. 큰 이미지 분류 또는 음성 인식과 같은 매우 복잡한 작업은 일반적으로 수십 개의 레이어가있는 네트워크 (또는 심지어 수백 개이지만 제 13 장에서 볼 수 있듯이 완전히 연결되지 않은 네트워크)가 필요하며 엄청난 양의 교육이 필요합니다 데이터. 그러나 그러한 네트워크를 처음부터 교육 할 필요는 거의 없습니다. 유사한 작업을 수행하는 미리 훈련 된 최첨단 네트워크의 일부를 재사용하는 것이 훨씬 더 일반적입니다. 교육은 훨씬 빨라지고 훨씬 적은 데이터가 필요합니다 (11 장에서 이에 대해 논의 할 것입니다).\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "대부분의 경우 숨겨진 레이어 (또는 11 장에서 볼 수있는 변종 중 하나)에서 ReLU 활성화 기능을 사용할 수 있습니다. 다른 활성화 함수보다 계산 속도가 약간 빠르며 그라디언트 하강은 큰 입력 값 (물류 함수 또는 쌍곡선 탄젠트 함수와는 반대로)에서 포화 상태가 아니므로 고원에 그리 많이 걸리지 않습니다. 1에서 포화 상태).\n",
    "\n",
    "출력 레이어의 경우 softmax 활성화 기능은 일반적으로 분류 작업 (클래스가 상호 배타적 인 경우)에 적합한 선택입니다. 회귀 작업의 경우 정품 인증 기능을 전혀 사용하지 않아도됩니다.\n",
    "\n",
    "이것으로 인공 신경망에 대한 소개를 마칩니다. 다음 장에서는 매우 깊이있는 네트워크를 훈련시키고 여러 서버와 GPU에 교육을 배포하는 기술에 대해 설명합니다. 그런 다음 우리는 몇 가지 다른 대중적인 신경 네트워크 구조, 즉 길쌈 신경 네트워크, 반복 신경 네트워크 및 자동 코드 작성기를 탐색 할 것입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
