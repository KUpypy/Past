{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`curse of dimensionality`: 차원이 많아질수록, 훈련이 매우 느리거나 좋은 솔루션을 찾기 힘들다. \n",
    "\n",
    "실제 세계의 문제에서는 다루기 힘든 문제들을 다룰 수 있는 것으로 바꿔, 변수의 갯수를 줄이는 것도 가능하다. 예를 들어 MNIST 이미지에서 보더가 항상 하얀색인 픽셀들은 아예 트레이닝 셋에서 뺄 수 있다(정보 잃지 않으면서). 또한, 두개의 이웃 픽셀중 상관성이 매우 높은 것들을 하나로 합치더라도, 정보를 크게 잃지 않으며 차원을 축소할수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차원 축소는 정보를 잃을수 있기 때문에 훈련 과정을 빠르게 할 수 있으며, pipeline을 좀 더 복잡하게 만들 수 있다. 따라서 먼저 기존 데이터로 훈련을 시켜 보는 것이 낫다. 하지만 몇몇 케이스에서는, 차원 축소가 노이즈와 필요 없는 디테일들을 제거해 더 높은 성능을 준다(일반적으로는 그렇지 않다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 차원 축소는 데이터 시각화에도 유용하다. 차원을 2~3차원으로 축소하면 고차원의 훈련셋을 그래프로 그려내 인사이트를 얻을 수 있기 때문이다.\n",
    "\n",
    "이 챕터에서는 차원의 저주를 설명하고, 차원 축소에 대한 접근법 두가지(projection & manifold learning)를 소개한다. 또한 PCA, Kernel PCA, LLE를 통해 차원을 축소하는 방법을 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 것들이 고차원 공간에서는 매우 다르게 행동한다. 예를 들어서, 1*1 unit square에서 랜덤한 포인트를 선택한다면, 이 포인트가 border로부터 0.001 이하로 위치할 확률은 0.4%이다. 하지만 10,000 차원의 hypercube에서 이 확률은 99.99999%보다 높다. 즉, 고차원 hypercube에서 거의 대부분의 포인트는 border에 가깝게 위치해 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, unit square에서 두 포인트의 평균 거리는 약 0.52이다. 3차원 큐브라면 이는 0.66이 되며, 백만차원 큐브에서는 408.25가 되버린다. (벡터의 차원이 늘어나니 당연) 즉, 데이터간의 거리가 매우 커져 버린다. 이러한 상황에서는 새로운 인스턴스 또한 훈련 인스턴스에서 멀게 위치할 가능성이 크다. 이는 예측을 저차원에서보다 덜 신뢰할수 있게 만든다. 즉, 고차원일수록 overfitting의 가능성이 크다.\n",
    "\n",
    "참고: https://dsp.stackexchange.com/questions/5967/why-transforming-the-data-to-a-high-dimensional-feature-space-in-which-classes-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_2.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이론적으로, 차원의 저주에 대한 솔루션은 트레이닝셋의 사이즈를 그에 해당하는 만큼 늘리는 것이다. 하지만 실제로 이는 거의 불가능 하다. 예를 들어 100차원이라고 하면 이를 위해 우주에서 관찰 가능한 원자의 수보다 더 많은 데이터가 필요하기 때문이다(모든 데이터를 서로 0.1 이내로 만들기 위해서는). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분 트레이닝 인스턴스는 모든 차원에 대해 uniform하게 흩어져 있지 않다. 많은 특성들은 거의 constant이지만, 나머지들은 상관성이 높다. 그 결과로, 모든 트레이닝 인스턴스들은 고차원 공간의 lower dimensional subspace안에 위치해 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_3.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 회색 space가 2차원 subspace이다. 만약 우리가 모든 트레이닝 데이터를 이 subspace에 수직으로 projection한다면, 밑의 그림과 같은 데이터셋을 얻게 된다. z1, z2가 이 새로운 차원의 축이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 projection은 밑의 경우에 통하지 않는다. 많은 케이스에서 subspace는 twist되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 단순히 면에 projection 시킨다면(x3을 없애서) 밑 사진 왼쪽처럼 될 것이다. 하지만, 우리는 오른쪽과 같은 데이터를 만들고 싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단히 말하자면, 2D Manifold는 고차원으로 휘어질 수 있는 2D shape이다. 일반적으로 말하자면, d-dimensional manifold는 n-dimensional space(d차원 hyperplane을 부분적으로 닮은, d < n)의 일부이다.\n",
    "\n",
    "많은 차원축소 알고리즘은 `manifold`를 모델링해 이루어 진다. 이를 우리는 Manifold Learning이라고 부른다. 이는 manifold assumption(hypothesis)-실세계의 고차원 데이터셋이 훨씬 더 낮은 차원의 manifold를 닮아 있다는-에 에 의존한다.\n",
    "\n",
    "MNIST 데이터셋을 생각해 보자: 모든 손필기 이미지는 어느정도 유사성을 가지고 있다. 그들은 선으로 이어져 있고, border는 하얀색이고, 등등. 만약 이미지를 랜덤하게 생성한다면, 그중 매우 조금만 손필기를 닮아 있을 것이다. 즉, 손필기 이미지를 생성하려고 할때의 degree of freedom은 아무 이미지나 생성해도 될 때보다 매우 낮다. 이러한 제약들이 데이터를 낮은 차원의 manifold로 만드는 경향을 가진다.\n",
    "\n",
    "manifold assumption은 하나의 다른 implicit assumption과 동반된다: classification or regression task가 저차원의 manifold 공간에서 이뤄질때 더 쉽게 된다는 것이 그것이다. 예를 들어, 밑의 그림에서 3차원일때의 decision boundary는 꽤 복잡하지만 2차원의 manifold space에서는 간단하다.\n",
    "\n",
    "하지만, 이러한 가정이 언제나 성립하지는 않는다. 예를 들어, 그림의 밑 부분 3차원 공간에서는 x1=5로 간단한 바운더리를 가지지만 이를 2차원으로 축소했을때는 더 복잡해진다. \n",
    "\n",
    "요약하면, 차원을 축소하는 것은 데이터의 형질에 달려있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_7.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 hyperplane을 정하고, 그다음 데이터를 여기에 projection한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving the Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 맞는 hyperplane을 정할 필요가 있다. 밑의 그림에서 볼 수 있듯, maximum variance를 가지는 축이 가장 좋은 축이다. 다른 projection보다 정보를 적게 잃기 때문이다. 다른 방법은, Mean squared distance를 최소로 하는 축을 고르는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_8.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA는 variance가 가장 큰 축을 찾는다. 그다음 두번째 축은 첫번째 것에서 orthogonal 하며, 남은 variance중 가장 큰 것이다. 이는 계속해서 이뤄진다(계속 이전 축들과 수직한 축을 찾음)\n",
    "\n",
    "i^th 축을 정의하는 unit vector는 i^th principal component라고 불린다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 principal component는 SVD(Singular Value Decomposition)을 통해 찾을 수 있다. 다음 코드로 이를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.matrix([[1,2,3],[4,5,6],[0.5,0.7,0.2],[8,3,1],[3,2,1],[1,2,3]])\n",
    "\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:,0]\n",
    "c2 = V.T[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA는 데이터가 원점을 중심으로 퍼져 있다는 것을 가정한다. Scikit-learn에서는 이를 해주지만, 다른 라이브러리를 쓴다면 먼저 데이터를 center 해야 한다는 것을 기억하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting Down to d Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 principal components를 찾았다면, 이를 통해 d principal components로 이루어진 hyperplane에 데이터들을 projecting 시켜 차원을 축소할 수 있다. \n",
    "\n",
    "training set을 hyperplane에 project하기 위해서는 단순히 training set matrix X 와 W_d(first d principal components)를 dot product 해주면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn의 PCA 클래스는 SVD를 우리가 했던 것과 동일하게 적용한다. 다음 코드로 실행시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 유용한 정보는 각 principal component의 explained variance ratio이다. 이는 데이터셋의 PC에 관한 variance의 비율을 알려준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6133419   0.38371093]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 정보는 61%의 데이터셋 variance가 첫번째 축에 있다(첫째 축이 61%의 variance를 설명)는 것이고, 38%는 두번째 축에 있다는 것이다. 남은 약 1%는 세번째 축이 될 수 있으나 이는 굉장히 작은 정도의 정보를 가지고 가는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 코드는 차원을 축소하지 않고 PCA를 한 후, 95%의 variance를 설명하기 위한 차원의 최소 숫자를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그다음 n_components=d 를 설정 후, PCA를 돌리면 된다. 참고로 이를 0.95 정도로 설정하면 바로 우리가 원하는 것이 실행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca= PCA(n_components = 0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_9.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 같은 경우 95% 로 PCA하면 feature가 784 -> 150개로 줄어듬. 또한 PCA projection의 inverse transformation을 이용해 다시 784개로 늘릴 수도 있다. 물론 이는 원래 데이터를 주지는 않는다(5%의 variance는 없어졌으므로) 하지만 꽤 비슷하게는 된다. 원래 데이터와 reconstructed data간의 mean squared distance는 `reconstruction error`라고 불린다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_10.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_11.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 훈련 데이터는 mini-batches로 나눠 IPCA 알고리즘에 하나씩 넣는 것이다. 이는 큰 훈련 데이터에 유용하며, PCA를 online으로 할수 있게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아니면 numpy의 memmap class 를 사용할수도 있다. 이는 디스크에 있는 바이너리 파일에 있는 큰 어레이를 을 메모리에 있는 것처럼 사용 가능하게 해준다. 이는 필요할때만 메모리에 필요한 데이터를 불러온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size) \n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 첫 d principal components에 대한 근사를 빨리 찾는 stochastic 알고리즘이다. 이는 O(m*d^2) + O(d^3) (원래는 O(m*n^2) + O(n^3)) 의 복잡도를 가져서, d가 n보다 크게 작을때 훨씬 빨라진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\") \n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH5 에서 우리는 kernel trick에 대해 배웠다. \n",
    "PCA에도 이런 트릭을 쓸 수 있다. 이는 Kernel PCA라고 불린다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, y = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n",
    "\n",
    "y = t > 6.9\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "for subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    if subplot == 132:\n",
    "        X_reduced_rbf = X_reduced\n",
    "    \n",
    "    plt.subplot(subplot)\n",
    "    #plt.plot(X_reduced[y, 0], X_reduced[y, 1], \"gs\")\n",
    "    #plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], \"y^\")\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_12.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kPCA는 비지도 학습 알고리즘으로써, 명백한 성능 평가 방법(best kernel, hyperparameter값 정하는)은 없다. 하지만, 차원 축소는 지도학습을 위한 준비 단계이고 따라서 커널과 hyperparameter를 정하기 위해 grid search를 이용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('kpca', KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',\n",
       "     fit_inverse_transform=False, gamma=None, kernel='linear',\n",
       "     kernel_params=None, max_iter=None, n_components=2, n_jobs=1,\n",
       "     random_state=None, remove_zero_eig=False, tol=0)), ('log_reg', LogisticRegre...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'kpca__gamma': array([ 0.03   ,  0.03222,  0.03444,  0.03667,  0.03889,  0.04111,\n",
       "        0.04333,  0.04556,  0.04778,  0.05   ]), 'kpca__kernel': ['rbf', 'sigmoid']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)), \n",
    "    (\"log_reg\", LogisticRegression()) ])\n",
    "\n",
    "param_grid = [{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"] }]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "완전히 비지도인 다른 방법은 reconstruction error를 가장 적게 하는 커널과 hyperparameter를 정하는 것이다. 하지만 reconstruction은 linear PCA처럼 쉽지 않다. 밑의 그림은 원래 swiss roll이다. 커널을 이용해, 훈련 셋을 무한차원 feature space로 매핑후 2차원 공간으로 projection 시키는 것은 동치이다. 만약 우리가 선형 PCA를 invert한다면(reduced space에 있는), recnstructed point는 feature space위에 있다(original space가 아닌). 여기서 feature space는 무한차원이므로, 우리는 reconstructed point를 계산할 수 없고 따라서 true reconstruction error를 계산할 수 없다. 다행히도, original space의 reconstructed point와 가까운 포인트를 찾을수는 있다. 이는 reconstruction `pre-image`라고 불린다. pre-image가 있다면, 오차를 계산할 수 있게 되고, 이를 최소화하는 kernel과 hyperparameters를 찾을 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch8_13.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 reconstruction을 하기위한 하나의 솔루션은 supervised regression model을 학습시키는 것이다. 여기서 projected 인스턴스는 훈련 셋, original 인스턴스는 타겟이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    "                    fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "auto",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
